---
title: "Time Series Project: Analysis of PM2.5 in Beijing"
author: "David Josephs"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
    number_sections: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    theme: paper
    df_print: paged
    keep_md: TRUE

---

```{r setup, echo = F}
if (knitr::is_latex_output()) {
	knitr::opts_chunk$set(dev = "tikz")
}
knitr::opts_knit$set(root.dir = '..')
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(tidy = T)
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(comment = '#>')
knitr::opts_chunk$set(fig.path = 'fig/')
options(scipen = 999)
```


# Project Setting

The setting for this project is as follows. You are an employee of the US Embassy in Beijing. After noticing the strange thickness in the air, and getting a bit of a cough, you decide that you do not what any other visitor to suffer breathing the highly dangerous Beijing air. So, you decide you want to get funding for the US Embassy to provide free air pollution masks to all visitors. However, the bureaucrats who can give you the money want to know two things: 

* How much money should we allocate to you over the next year

* How many air pollution masks should be made available each day over the course of the year (rush shipping is expensive)

So, your task is to predict the air pollution levels each day in Beijing at the US Embassy (who records it every hour), so that you can give the bureaucrats an actionable plan. We are assuming that more people will come in to get their free mask on a day where air pollution is bad, than on a day where it isnt, and we certainly do not want to run out, as that would be unfair.

Given this setting, we will now discuss how the data was processed:

# Data Pre-processing

## Data Definition

Open to the public is data from 5 cities:

* Beijing

* Chengdu

* Guangzhou

* Shanghai

* Shenyang

Each of these datasets contains:

* Air quality measurements at multiple locations (including a US Embassy post)

* Humity

* Air pressure

* Temperature

* Precipitation

* Wind direction

* Wind Speed

Measured at every hour. For detailed info on the data quality and collection methods, please refer to [this publication](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2016JD024877)


## Pre-processing pipeline

The next step in our analysis workflow is to get the data ready for analysis. First, we will have to do a few tasks

* import data

* Represent data as a time series

* inspect for NAs

* figure out how to deal with NAs

* store in a conveniently manipulatable format

All of this can be seen in `R/preprocessing.R`, but we will do an in depth explanation here

### Required libraries

```{r datalibs}
library(functional) # to compose the preprocessing pipeline
library(data.table)# to read csvs
library(rlist) # for list manipulations
library(pipeR) # fast, dumb pipes
library(imputeTS) # to impute NAs
library(pander) # so i can read the output
library(foreach) # go fast
library(doParallel) # go fast
```

### Data loading

We want to load all of the csv files stored in `data/`, so we will write a function which loads all of them into a list of `data.table`(fast data frame) objects. We will want the name of each item in the list to be the name of the csv file, minus the useless numbers

```{r import}
import <- function(path){
  # first we list the files in our path, in this case "data/"
  files <- list.files(path)
  # then we search for csvs
  files <- files[grepl(files, pattern = ".csv")]
  # paste the csv names onto the filepath(data/whatever.csv)
  filepaths <- sapply(files, function(x) paste0(path,x))
  # read them in to a list
  out <- lapply(filepaths, fread)
  # Get rid of .csv in each filename
  fnames <- gsub(".csv","",files)
  # get rid of the confusing numbers
  fnames <- gsub("[[:digit:]]+","", fnames)
  # set the names of the list to be the clean filenams
  names(out) <- fnames
  out
}
```

### NA identification

Next we want to see how many NAs we are dealing with. Since we have about 20 million total observations, we will represent these as a proportion. We will apply the NA count function to our list using the beautiful `rapply` function

```{r naid}
naCount <- function(xs){
  rapply(xs, function(x) sum(is.na(x)/length(x)), how = "list")
}
# load in the data now
datadir = "data/"
china <- import(datadir)
pander(naCount(china))
```

It looks like the quality of the data collected outside of the US Embassy posts and in cities other than beijing is really bad. Luckily for us, `BeijingPM$PM_US.Post` has relatively high quality data. That being said, we are still going to have to impute the NAs. Before that though, lets convert this to time series data, as we will be imputing the NAs in a different way with time series data than we will for normal data.

### Data transformation: Hourly time series

First, we define a function which converts the data to a time series object:

```{r tots}
tots <- function(v){
  ts(v, frequency = 365*24)
}
```

Next, we are going to want want to scale this up, to work on a whole data frame. We will also convert the data frame to a list, just to avoid any typing issues down the road. We also do not want to convert non time series data to time series, so we must avoid those as well:

```{r}
totslist <- function(df){
  # define the column names which we do not want to alter
  badlist <- c(
               "No",
               "year",
               "month",
               "day",
               "hour",
               "season",
               "cbwd"
  )
  # get the column names of the original data frame
  nms <- colnames(df)
  # coerce it into a list
  df <- as.list(df)
  # if the name of the item of the list is in our
  # to not change list, do nothing
  # otherwise, convert to ts
  for (name in nms){
    if (name %in% badlist){
      df[[name]] <- df[[name]]
    } else {
      df[[name]]  <- tots(df[[name]])
    }
  }
  # return the created list
  df
}
```

Next, we want to scale this function to work on a list of data frames:

```{r, listofts}
totsall <- function(xs){
  lapply(xs, totslist)
}
```

### NA imputation

Imputing NAs in time series is a tricky topic, as we cannot just ignore them (that would mess up our sampling rate). There are several methods of dealing with NAs in time series, including kalman filters, polynomial interpretation, and spline interpolation. In this case, as we are only concerned with the time series that have relatively few NAs, I elected to use spline interpolation, as it is much faster than the kalman filter, while still providing pretty good accuracy. First, a function was constructed to use the try catch pattern to impute NAs of items with the `time-series` class.

The logic is as follows:

* try spline interpolation on the vector
  * if the output is a `ts` object, return the results
  * if the output is an error or a non time series object, return the original object

```{r trycatch}
imp_test <- function(v){
  out <- try(na.interpolation(v, "spline"))
  ifelse(
         is.ts(out),
         return(out),
         return(v)
  )
}
```

Next, we apply that function to a single list, in parallel to save time:

```{r partry}
impute <- function(xs){
  foreach(i = 1:length(xs),
          .final = function(x){
            setNames(x, names(xs))
          }) %dopar% 
  imp_test(xs[[i]])
}
```

Finally, we apply that function to our list of lists

```{r lol}
impute_list <- function(xs){
  lapply(xs, impute)
}
```

Next we are ready to convert to a more useful data structure, and then pipeline all these functions together for quick preprocessing

### Pipelining

First, we will convert our slow, massive list into the fast, superior hash table. For large, complex datasets, a hash table was shown to be at more or less 3 times faster than a list, and as we will be repeatedly accessing data from this, we would prefer it to be a quick index. For more info on R hash tables, please refer to [this link](https://blog.dominodatalab.com/a-quick-benchmark-of-hashtable-implementations-in-r/).

```{r tohash}
to_hash <- function(xs){
  list2env(xs, envir = NULL, hash = TRUE)
}
```

Univariate EDA
Next we are ready to pipeline it all together into a single preprocessing function, using `functional::Compose`

```{r compose}
preprocess <- Compose(import, totsall, impute_list, to_hash)
```

# Univariate EDA

Required libraries

```{r}
library(ggthemes)
library(ggplot2)
library(cowplot)
```

## Helper functions

First, we define some helper functions, in order to save us time and make our code more readable. First, lets steal some stuff from `forecast` and `tswge`:

```{r thief}
# pretty plots for our analysis
seasonplot <- forecast::ggseasonplot
subseriesplot <- forecast::ggsubseriesplot
lagplot <- forecast::gglagplot
sampplot <- tswge::plotts.sample.wge
# clean up errant data points
clean <- forecast::tsclean
```

Next let us define a function which creates a function to resample time series, and then define ones with new sampling rates. Finally, we will write a function which applies each resampling function to a vector

```{r resamp}
# resample generator
change_samples <- function(n){
  function(xs){
    out <- unname(tapply(
                         xs,
                         (seq_along(xs)-1) %/% n,
                         sum
                         ))
    out <- ts(out, frequency = (8760/n))
    out
  }
}

# daily and weekly sampling, monthly is 4 weeks
to_daily <- change_samples(24)
to_weekly <- change_samples(24*7)
to_monthly <- change_samples(24*7*4)
to_season <- change_samples(24*(365/4))

# pipelining final cleaning and conversion, removing outlier and the couple of errant negative values (which do not make sense)
cleandays <- function(xs) {
  xs %>>% clean %>>% abs %>>% to_daily
}

cleanweeks <- function(xs) {
  xs %>>% clean %>>% abs %>>% to_weekly
}
cleanmonths <- function(xs) {
  xs %>>% clean %>>% abs %>>% to_monthly
}
cleanseas <- function(xs) {
  xs %>>% clean %>>% abs %>>% to_season
}

# resample the whole ts 
resample <- function(xs){
  xs %>>% cleandays %>>% window(end = 6) -> day
  xs %>>% cleanweeks %>>% window(end = 6) -> week
  xs %>>% cleanmonths %>>% window(end = 6) -> month
  xs %>>% cleanseas %>>% window(end = 6) -> seas
  list(day = day, week = week, month = month, season = seas)
}
```

## Wge sample plots

First lets load in and resample our data:

```{r bjsample}
library(ggthemes)
library(ggplot2)
library(cowplot)
china <- preprocess(datadir)

bj <- china$BeijingPM_$PM_US
bj <- resample(bj)
```

### Daily data

Now lets look at the sample plots, first with the daily data

```{r}
dy <- sampplot(bj$day)
```

So looking at this, we see that we have what looks like a wandering behavior, according to the frequency plot, but the lags tell a different story. 
There appears to be some strong, high order ARMA stuff going on. Looking at the line plot, we see that there looks to be a long seasonal component, maybe a yearly one, which would explain why the Parzen window is telling us its somewhat wandering. 
We also see what looks to be a few more peaks in the parzenwindow, which either tells us that there is a complex seasonality going on (likely at this sampling rate), or we have some sort of high order ARMA model (or potentially both). 
Lets widen our sampling rate to weeks, months, and quarters, to see if there is anything gong on there, maybe be able to catch that yearly pattern. Lets check it out:


So lets go through each of these plots and discuss:

### Weekly data

```{r}
wy <- sampplot(bj$w)
```

Here we can see better that the time series is not so much wandering, as having a very long seasonal pattern (parzen window). We can tell by the little hook on the left. The ACFs are not so useful in this case, as they are tiny beyond lag 1, due to our weird sampling. However, we know for sure there is some sort of seasonal pattern going on. Potentialy there is a multiseasonal pattern again, or some high order ARMA. 

### Monthly data

```{r}
my <- sampplot(bj$m)
```

Here we see another oscillating component in the ACFs, indicating again seasonal or high order ARMA (it looks a lot like patemp from tswge, which is seasonal data that is well described by a high order ARMA model). The frequency is again painting a similar picture, there is something going on with a clear frequency. lets look at the quarterly data to know for sure

### Quarterly Data

```{r}
sy <- sampplot(bj$s)
```

With a bit more than 20 data points, and a supposed seasonal period of a year (4 datapoints), we would *expect* to have a frequency of about 0.2, and lo and behold, we have one. There is some strong evidence of a yearly seasonal pattern in the data (also look at the peak in the ACFs at 4, pretty good evidence right there). The scatterplot tells us a pretty obvious story as well

## Seasonal Plots

Lets check out this seasonal pattern a bit more with a seasonal plot. We will plot them all at once this time and then discuss:

```{r}
sda <- seasonplot(bj$day) + 
  scale_color_hc() + theme_hc() + 
  ggtitle("Seasonal plot: Daily")
sdw <- bj$week %>>% seasonplot + 
  theme_hc() + scale_color_hc()+
  ggtitle("Seasonal plot: Weekly")
sdm <- bj$month %>>% seasonplot + 
  theme_hc() + scale_color_hc()+
  ggtitle("Seasonal plot: Monthly")
sds <- bj$seas %>>% seasonplot + 
  theme_hc() + scale_color_hc()+
  ggtitle("Seasonal plot: Quarterly")
plot_grid(sda, sdw, sdm, sds, ncol=2 )
```

### Interpretation

The shape of these seasonal plots, and how they all kind of line up, indicate to me that there is some sort of seasonality. It is especially apparent in the Monthly plot, where we have it line up very well, but it is clear in the other plots as well.

> Why is this happening? Is there a real world explanation to this?

Indeed there is. Every year, around November, citywide central heating turns on inBeijing for the winter. This would cause a clear change in the air pollution, as heating is not an energy efficient process. In the summer, it goes down, because central heating is off, and people are more likely to go outside and walk and open their windows.

# Classical Univariate Analysis

With our newfound knowledge, we will now perform a classical analysis of this time series.

```{r}
library(tswgewrapped)
#> my wrapper scripts of tswge, with more convenient syntax
```

## Setup

The setup for this bit of analysis is as follows:

```{r bjday}
china <- preprocess("data/")
bj <- china$BeijingPM_$PM_US
bjs <- resample(bj)
bjus <- bjs$day

# split
train <- window(bjus, end = 5)
test <- window(bjus, start = 5)

# clean up
rm(bj, bjus, bjs)
```

## S3 objects and methods

To make analysis of models from different libraries with different structures easier, I wrote some S3 classes and generic methods in order to view and evaluate them. These are shown below

### Scores generic

The scores generic is used to get the score of a model of any type. 

```{r scorem}
scores <- function(obj){
  UseMethod("scores")
}
```

### The wge class

We next create a `wge` class, where we can store our tswge forecasts

```{r}
as.wge <- function(x) structure(x,class = "wge")
```


We will use 3 methods to evaluate TSWGE models:

#### ASE

```{r asefun}
ASE <- function(predicted, actual){
  mean((actual -predicted)^2)
}
```

#### MAPE

```{r mapefun}
MAPE <- function(predicted, actual){
  100*mean(abs((actual-predicted)/actual))
}
```

#### Confidence Score

Confidence score is a made up metric I created, in order to evaluate the prediction interval. How it works is as follows: For each point in the actual observations, if it lies outside the prediction interval, give that point a score of 1. If it is within the confidence interval, give it a score of 0. Then, average the scores and multiply by 100 to get the percentage we got wrong.

```{r confun}
checkConfint <- function(upper,lower, actual){
  (actual < lower) | (actual > upper)
}

confScore <- function(upper, lower, actual){
  rawScores <- ifelse(
    checkConfint(upper,lower,actual),
    1,
    0
  )
  return(sum(rawScores)/(length(actual))*100)
}
```

#### Method for scoring wge objects

To score tswge objects, we can simply use the following S3 method:

```{r}
scores.wge <- function(xs){
  mape <- MAPE(xs$f,test)
  ase  <- ASE(xs$f,test)
  confs <- confScore(xs$ul, xs$ll,test)
  c("ASE" = ase, "MAPE" = mape,  "Conf.Score" = confs)
}
```

### Visual comparison of prediction and reality

To compare the predictions vs reality of a wge object, I wrote an autoplot method for wge objects:

```{r}
.testPredPlot <- function(xs){
  p <- ggplot() + theme_hc() + scale_color_hc()
  doplot <- function(df){
    p <<- p + geom_line(data = df,
                        aes(
                            x = t,
                            y = ppm,
                            color = type
                            ))
  }
  out <- lapply(xs, doplot)
  out[[2]]
}


autoplot.wge <- function(obj){
  testdf <- data.frame(type = "actual", 
                       t = seq_along(test), 
                       ppm = as.numeric(test))
  preddf <- data.frame(type = "predicted", 
                       t = seq_along(obj$f), 
                       ppm = as.numeric( obj$f ))
  confdf <- data.frame(upper = obj$ul, lower = obj$ll, t = seq_along(test))
  dfl <- list(testdf,preddf)
  .testPredPlot(dfl) + geom_line(data = confdf,
                                 aes(x = t, y = lower, alpha = 0.2), linetype = 3003) + geom_line(data = confdf,
                                 aes(x = t, y = upper, alpha = 0.2), linetype = 3003) + guides(alpha = FALSE)
}

```

## Differencing

Before we begin our analysis, we will create a seasonal and trend based difference of our original time series, mostly for the sake of thouroughness. The difference function used is as follows

```{r}
difference
```

### Seasonal differencing

Our EDA tells us there should be a seasonality of ~365 days. So, lets take that out of our time series.

```{r}
trainSea <- difference(seasonal, train, 365)
```

Nothing appeared to happen, This is either because the difference is out of the scale of our ACF plot, (365 is a lot of lags), or because there is something more going on. We will see.


### Trend based differencing

Just for the sake of completeness, lets take a trend based difference of our data:

```{r}
trainTrend <- difference(arima, train,1)
```

There is little to say here, I do not believe in this model.

## Estimation of order

To estimate the order, first all the train sets were put into a list:

```{r, eval = F}
trainers <- list("ARMA" = train, "Seasonal" = trainSea,"ARIMA"=trainTrend)
```

Then, a wrapper around aic.wge speed up (especially with parallel computation) aic5.wge, was used on the trainers. The speed differences are minor but still much time is saved. The wrappers are:

```{r}
tswgewrapped:::expand
tswgewrapped:::rewrite
tswgewrapped:::getpq
aic5
tswgewrapped:::aics
aicbic
``` 

The rest of the source code can be seen [here](https://github.com/josephsdavid/tswgewrapped). The calculation of all the AICs and BICs is shown below

```{r, eval = F}
aics <- lapply(trainers, aicbic, 0:10, 0:8)
```

```{r, echo = F}
load("analysis/daily/aics.Rda")
```

```{r}
pander(aics)
```

## Estimation of Parameters

To estimate the parameters, we will be using the wrapper estimation function I wrote (just more convenient syntax), as well as the wrapper around ljung.wge I wrote, which is just more convenient to read (and doesnt print 100 residuals)

```{r}
estimate
tswgewrapped:::silence
hush

ljung_box
```
All estimations were run with different orders until the best results were found:

### ARMA estimates

```{r}
estARMA <- estimate(train, 9, 8)
ljung_box(estARMA$res, 9,8)
par(mfrow = c(1,2))
acf(estARMA$res)
plot(estARMA$res)
```

### ARUMA estimates

```{r}
estSeas <- estimate(trainSea,7,6)
ljung_box(estSeas$res,7,6)
par(mfrow = c(1,2))
acf(estARMA$res)
plot(estARMA$res)
```

### ARIMA estimates 

```{r}
estTrend <- estimate(trainTrend,6,3)
ljung_box(estTrend$res, 6,3)
par(mfrow = c(1,2))
acf(estARMA$res)
plot(estARMA$res)
```

### Discussion

All of our models look pretty good as far as estimates go. However, I think only ARUMA and ARMA are appropriate. We are only proceeding with ARIMA to be thorough.

## Forecasting

We will now proceed with our daily, one year ahead forecasts, as well as the evaluation of them. The `fcst` function from `tswgewrapped` is again just a wrappper for `fore.whatever.wge`:

```{r}
tswgewrapped::fcst
```

### ARMA

```{r}
armaCast <- tswgewrapped::fcst(type = aruma, 
                     x = train, 
                     phi = estARMA$phi,
                     theta = estARMA$theta, 
                     n.ahead = length(test)) %>>% as.wge
```

This one damps to the mean pretty quickly, however this may be a decent predictor, it is common sense that most likely the result will be the mean. It may provide a good baseline in the future, but is not in its own right a very good predictor

### ARUMA

```{r}
seaCast <- tswgewrapped::fcst(type = aruma, 
                    x = train, 
                    phi = estSeas$phi, 
                    theta = estSeas$theta, 
                    s = 365, 
                    n.ahead = length(test)) %>>% as.wge
```

This looks a lot more like the time series, so it is likely a more appropriate model at this forecast horizon, but the forecasts themselves appear to be off

### ARIMA

This will just be the last value over and over, highly uninteresting and not appropriate

```{r}
trendCast <- tswgewrapped::fcst(type = aruma, 
                    x = train, 
                    phi = estTrend$phi, 
                    theta = estTrend$theta, 
                    d = 1, 
                    n.ahead = length(test)) %>>% as.wge
```

Nothing surprising here

#### Note

An airline model was not attempted, because it does not make much sense.

## Model evaluation

### Scores
As the appropiateness of each model has already been noted, so now we can look at the scores of each model:

```{r}
casts <- list("arma" = armaCast, "seasonal" = seaCast, "arima" = trendCast)
data.frame(list.rbind(lapply(casts,scores)))
```

These results are not surprising, our common sense ARMA forecast in the long run did pretty well (just the mean), and will serve well as a benchmark for future forecasts. Our seasonal model, which looks pretty, did not do so hot, as it was just off a bit. Our ARIMA model only did well because the last value was near the mean. Below we can see the comparison between the train and test:

```{r, fig.show = "hold"}
autoplot(armaCast) + ggtitle("ARMA")
autoplot(seaCast) + ggtitle("Seasonal")
autoplot(trendCast) + ggtitle("ARIMA")
```

One important thing to note is the prediction intervals for these models. They did an excellent job of capturing the actual data within their prediction intervals. This is really useful information to us.

## Discussion

Although our forecasts were not very good, especially our ARMA forecast will provide us with a nice benchmark to compare our newer models to, at least by ASE. I think the seasonality is an important factor in this dataset, and although our seasonal model did not do so well, it is worth looking more into. We will proceed next with some multiseasonal approaches. As it is daily data, there tend to be two major seasonalities: a weekly pattern, and a seasonal pattern. 

## Saving the important data:

We would like to save the important information from our models, so that way in the end, we have a nice list of all of our information. That is, the scores of the model, the predictions themselves, and the the prediction limits:

```{r}
makeModel <- function(...){
  UseMethod("makeModel")
}

makeModel.wge <- function(obj){
  score <- scores(obj)
  return(list(
              preds = obj$f,
              ul = obj$ul,
              ll = obj$ll,
              ase = score[1],
              mape = score[2],
              Conf.Score = score[3]
        ))
}
models <- lapply(casts, makeModel)
```

# Multiseasonal setup

First, we will load up the `forecast` library, in order to get functionality for multiseasonal forecasts

```{r}
library(forecast)
```

Next, we will recast our train and test sets as msts objects:

```{r msrecast}
periods  <- c(7, 365.25)
train <- msts(train, seasonal.periods = periods, end = 5)
test <- msts(test, seasonal.periods = periods, start = 5)
```

# Multiseasonal Dynamic Harmonic Regression

In class, we learned that you can perform standard regression on a time series, then apply our arima analysis on the errors of that time series, and then adjust for that. The `auto.Arima` function does all of this for you, if you supply it with an x regressor. The math behid it is basically representing the equation as follows

$$y_t = \beta_0 + \beta_1 x_{1,t} + ... + \beta_k x_{k,t} + \eta_t$$

where $\eta_t$ is a time series of errors. By estimating that time series, we can then properly estimate $y_t$, as well as make forecasts.

In the case of multiple seasonalities, we can get a bit more creative. We can represent the time series as a fourier series, that is we can write a bunch of sine and cosine terms, a set for our weekly seasonality, and a set for our yearly seasonality. We can see this below:

```{r}
exampleFourier <- data.frame(fourier(train, K = c(3,100)))
```

Note that we do three fourier expansions of our weekly trend, and about 100 of our yearly trend. We want to do about half as many expansions as our trend. Lets check out what they look like, we will look at one fourier expansions on a weekly level, and one on a yearly level

```{r wview}
weekly <- exampleFourier[[1]]  + exampleFourier[[2]] + 
  exampleFourier[[3]] +exampleFourier[[4]] + 
  exampleFourier[[5]] + exampleFourier[[6]]
par(mfrow = c(1,2))
plot(weekly[1:63], type = "l")
plot(train[1:63], type = "l")
```

So we see we may not have all of the patterns of the data on a weekly scale represented, but it is not a half bad representation. Let us try with the yearly data as well(we will provide a smoothing on the weekly level first so we can see)

First lets do a low pass filter of our training dataset:

```{r}
mafun <- function(xs, n) {
    stats::filter(xs, rep(1, n))/n
}
trsmooth <- mafun(train, 56)
```

```{r yview}
yearly <- exampleFourier[[7]]  + exampleFourier[[8]] + 
  exampleFourier[[9]] +exampleFourier[[10]] + 
  exampleFourier[[11]] + exampleFourier[[12]]
par(mfrow = c(1,2))
plot(yearly, type = "l")
plot(trsmooth, type = "l")
```

Again, not a bad little representation of the series. If we incorporated the remaining `r ncol(exampleFourier - 12)` components of our fourier expansion of the yearly trend, we would have a really powerful representation of the 365.25 day period.

We can use this fourier expansion as a regressor on our time series, and this should give us a pretty good representation of the complex seasonality of this series.

## S3 methods

We need to construct a few S3 methods for this section. First, the `forecast` function for `Arima` objects (those constructed by `auto.arima` and `Arima`) returns a forecast of the same length as the length of the external regressors (regardless of the value of `h`).
To deal with this, we write a new method called `newFore`, which creates a new `Arima` object that can make forecasts off the length of the test set, and then does a forecast using those xregressors. This returns the same value as running:

```{r, eval = F}
predict(`Arima object`, newxreg = `regressors from the test set`, n.ahead = whatever)
```

but gives us the benefits of the `forecast` object, such as nice plots, confidence intervals, etc.

```{r newfore}
newFore <- function(...){
  UseMethod("newFore")
}
newFore.Arima <- function(obj, newdata, xreg = NULL, h = 1){
  refit <- Arima(newdata, model = obj, xreg = xreg)
  forecast(refit, h = h, xreg = xreg)
}
```

Next we want to add new methods for these forecast objects, so we can view our results in a consitent manner:

```{r}
as.fore <- function(x) structure(x, class = "fore")

autoplot.fore <- function(obj){
  testdf <- data.frame(type = "actual", 
                       t = seq_along(test), 
                       ppm = as.numeric(test))
  preddf <- data.frame(type = "predicted", 
                       t = seq_along(test), 
                       ppm = as.numeric( obj$fitted[1:length(test)] ))

  confdf <- data.frame(upper = obj$upper[,2], lower = obj$lower[,2], t = seq_along(test))
  dfl <- list(testdf,preddf)
  .testPredPlot(dfl)+ geom_line(data = confdf, aes(x = t, y = lower, alpha = 0.2), 
         linetype = 3003) + geom_line(data = confdf, aes(x = t, y = upper, alpha = 0.2), 
         linetype = 3003) + guides(alpha = FALSE)
 }




scores.fore <- function(obj){
  mape <- MAPE(obj$fitted[1:length(test)], test)
  ase <- ASE(obj$fitted, test)
  confs <- confScore(obj$upper[1:length(test),2], obj$lower[1:length(test),2], test)
  c( "ASE" = ase, "MAPE" = mape,"Conf.Score" = confs)
}
```

## Building the model

```{r, echo = F}
load("analysis/daily/mseadyn.Rda")
```

First we construct a fourier expansion of both the train and test set:

```{r}
trainExp <- fourier(train, K = c(3,100))
testExp <- fourier(test, K = c(3,100))
```

Next, we plug this in to the auto.arima function, and build our model. Note we are passing the parameter `lambda = 0` into the function. Internally, auto.arima does a Box-Cox transformation on the data before running it, to make the model work smoothly. In our case, we do not want values less than zero, because that wouldnt make sense. So, we set `lambda`, the parameter passed into the box cox transformation to 0, which represents a logarithmic transformation. In turn, this will keep our model, and thus our predictions

```{r, eval = F}
mseadyn <- auto.arima(
                      train,
                      seasonal = FALSE,
                      lambda = 0,
                      xreg = trainExp
)
```

Lets check out where the roots lie on the unit circle:

```{r}
autoplot(mseadyn)
```

And lets see what order model it identified within the residuals:

```{r}
data.frame(
           p = length(mseadyn$model$phi),
           q = (mseadyn$model$theta),
           d = length(mseadyn$model$Delta)
)
```

## Forecasting

First, lets look at what the origina forecast looks like, without doing the newFore function:

```{r}
forecast(mseadyn, xreg = trainExp) %>>% autoplot
```

Whew, 5 years ahead. Thats a bit ridiculous.
Now lets run the newFore on it, and then check it out again

```{r}
mseafor <- newFore(mseadyn, newdata = test, xreg = testExp, h = 1)
autoplot(mseafor)
```

Note the scale on this plot it is now a one year ahead forecast, which is exactly what we want. Unlike the basic ARUMA forecast we did earlier, the confidence limits also lie above zero in all cases, and hopefully it will be a bit better.

## Model Evaluation

This model seems to be slightly more appropriate than the simpler models we fit, as it is highly likely this data is multiseasonal, and I much prefer having a lower ARMA order than (9,8). Lets check it out

### Visual inspection

```{r}
mseafor %>>% as.fore %>>% autoplot
```


This appears to be a ridiculously good forecast. Lets check out how it scored:

### Scores

```{r}
mseafor %>>% as.fore %>>% scores %>>% t  %>>% data.frame
```

Wow. The ASE is nearly a million units smaller than the baseline ARMA forecast. This is a massive improvement, and it seems that our hypothesis of multiseasonality in the data was correct. Lets try a different multiple seasonality technique. Aside fro that, a good percentage of the data was stored in our prediction interval. This indicates that on its own, this model is a good forecasting tool.

## Model saving

We will now add this model to our list of models:

```{r}
makeModel.fore <- function(obj){
  score  <- scores(obj)
  return(list(
              preds = obj$fitted,
              ul = obj$upper[,2],
              ll = obj$lower[,2],
              ase = score[1],
              mape = score[2],
              Conf.Score = score[3]
              ))
}

models$dynamic <- makeModel(as.fore(mseafor))
```

# TBATS 

TBATS is a new (to me) and exciting method for dealing with time series with complex seasonalities. TBATS stands for: ***Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend and Seasonal components***

> What?

Basically, the TBATS algorithm is going to:

* Perform a box-cox transformation on the time series

* Break it into Seasonal, ARMA (noise/residual), and Trend (with damping) components

* Estimate the Seasonal component(s) as a fourier series

* Estimate the trend component

* Estimate the ARMA component

* Combine everything back together to represent the series

> How is this different from what we just did

Unlike what we just did with dynamic harmonic regression, where the seasonality was fixed (as it is simply read in from the attributes of the `msts` object), seasonality here is allowed to change over time, as it is calculated dynamically from the data. This means that that pattern we saw in our EDA in the quarterly season plots can be maybe accounted for (where year one and two had a different pattern than the rest of the years). 

> Why havent i heard of this

Unlike a lot of time series techniques, tbats is from the 21st century, and not well proven. It also has a hard time sometimes, as it is an automated modeling tool, so some special cases will mess it up. It also is exceptionally slow, without using parallel processing it took about 20 minutes to run, on this small amount of data. Despite this, one of its great strengths is its automation, making it very easy to get a pretty good model, a lot of the time.

Lets go ahead and get into it.

## S3 methods

`tbats` comes from the `forecast` package, and has similar issues with forecasting as we saw above (its forecasts are the same length as the data it was trained on, `h` is ignored). So lets write a newFore method for `bats` objects

```{r newbat}

newFore.bats <- function(obj, newdata, h = 1){
  refit <- tbats( model = obj,y  = newdata)
  forecast(refit,  h)
}
```

Thankfully, the structure of a tbats forecast is exactly the same as the structure of our dynamic regression forecasts, so for the rest of the methods, we can just coerce our tbats forecasts into `fore` objects.

## Model Building

First, lets note whether train and test are msts or ts objects, just for sanity

```{r}
sapply(list(train,test), class)
```

Ok great we are good to go.

```{r, eval = F}
cores <- parallel::detectCores()
#> [1] 12
bjbats <- tbats(train, use.parallel = TRUE, num.cores=cores-1)
```

```{r, echo = F}
load("analysis/daily/tbats.Rda")
```

```{r}
autoplot(bjbats)
```

This is interesting too, so we see the I think trend that the algorithm smoothed out of the time series. Lets go ahead and make the forecast and see how we did

## Forecasting

Since we already know what will happen if we just make a forecast with the `forecast` function, lets instead use our `newFore` method without comparison

```{r}
batF <- newFore(obj = bjbats,  test, h = 366)
autoplot(batF)
```


Interesting. What is the autoplot showing us? It is a good question. It is hard to say whether this model is any good or not just from this. Lets try looking at the actual forecasted values with our autoplot method

## Model Evaluation

### Visual Inspection

```{r}
forbats <- as.fore(batF)
autoplot(forbats)
```

The autoplot of the forecast itself was strange, but looking at the fitted values, it was a pretty dang good forecast. 
It is interesting how both of these multiseasonal models captured the shape of the data nicely (see around the 100 point mark), but not the scale. 
TBATS did an exceptional job of getting the shape of the data (even more than our dynamic regression, see around `t = 200`), but it did worse as far as scaling goes.

Lets check out the scores:

### Scores

```{r}
scores(forbats) %>>% data.frame
```

By the numbers, this is an exceptional model as well (or at least better than our naive baseline). As far as appropriateness goes, it is hard to say as the TBATS algorithm is a bit black box. However, it got a pretty good forecast and looks similar to our original data, so I will give it a pass. I believe the fact that the seasonality can change is what made this a bit better than our regression with ARMA errors. As far as prediction intervals go, this model managed to capture even further the swings of the data.

## Saving the Forecast

```{r}
models$tbats <- makeModel(forbats)
```

# Prophet Algorithm

Facebook released their internal fast and easy forecasting algorithm, `prophet`.
The prophet algorithm works in the following manner:

* It decomposes the time series into seasonal, trend, and holiday components.

* The seasonal component is estimated as in TBATS, with fourier series. By default, it looks for a daily (if we have sub daily data), weekly, and yearly trends, but these can be specified (for example we can include a 5 day work week). These fourier series are then multiplied by a normal distribution with the same variance as our data.

* The trend component is modeled basically as logistic growth. However, it is not just simple logistic growth, they use piecewise logistic growth, which means that the trend component is allowed to change with time. 
  * If the trend does not follow a logistic growth pattern, it also checks for a linear trend with changepoitnts (aka a piecewise function).

* If  a list of holidays and events with their dates are provided, a matrix of dummy variable regressors for them is added to the model.

* Components are combined as a general additive model (GAM), and then you are done

## S3 methods

We will give the usual S3 methods. Because prophet forecasts include the entire dataset, we will have to write methods that take out the training parts. First, we define the coercion method

```{r}
as.proph <- function(x) structure(x, class = "proph")
```

Next we define our autoplot and scoring methods:

```{r}
scores.proph <- function(obj){
  c(
    ase = ASE(obj$yhat[-(1:length(train))], test),
    mape = MAPE(obj$yhat[-(1:length(train))], test),
    Conf.Score = confScore(upper = obj$yhat_upper[-(1:length(train))], lower = obj$yhat_lower[-(1:length(train))], test)
  )
}
autoplot.proph <- function(obj){
   testdf <- data.frame(type = "actual", 
                        t = seq_along(test), 
                        ppm = as.numeric(test))
   preddf <- data.frame(type = "predicted", 
                        t = seq_along(test), 
                        ppm = as.numeric( obj$yhat[-(1:length(train))] ))
   confdf <- data.frame(t = seq_along(test), upper = obj$yhat_upper[-(1:length(train))], lower = obj$yhat_lower[-(1:length(train))])
   dfl <- list(testdf,preddf)
   .testPredPlot(dfl)+ geom_line(data = confdf, aes(x = t, y = lower, alpha = 0.2), 
         linetype = 3003) + geom_line(data = confdf, aes(x = t, y = upper, alpha = 0.2), 
         linetype = 3003) + guides(alpha = FALSE)

 }
```

## Building a model

First, we need to load the data in the proper format

```{r}
st <- as.Date("2010-1-1")
en <- as.Date("2013-12-31")
trainDates <- seq(st,en, by = "day")
# data MUST BE IN THIS EXACT FORMAT
traindf <- data.frame(ds = trainDates, y = as.numeric(train))
```

Next, lets make our model. This is pretty easy

```{r, message = F, warning = F}
library(prophet)
model <- prophet(traindf)
```

## Forecasting

First, we must make a data frame for the future, because prophet has a silly API

```{r}
future <- make_future_dataframe(model, periods = 366)
```

Now we can forecast, with the `predict` function

```{r}
fore <- predict(model,future)
plot(model,fore)
```

We can also view how it modeled/forecasted all the components

```{r}
prophet_plot_components(model, fore) + theme_hc()
```

## Model Evaluation

Lets check out how we did. First, lets take a look at our forecast:

```{r}
pfore <- as.proph(fore)
autoplot(pfore)
```

This is a pretty weird little forecast. Because of the smoothing, and the normalized nature of all the prophet algorithm (it is much closer to regression than anything else), it captures the shape of the series well, but it does not get the extremety of the series. This model may be good for detecting overall patterns in the data, but not for specific predictions. Lets check out the scores.

```{r}
data.frame(t(scores(pfore)))
scores.proph
# function(obj){
#   c(
#     ase = ASE(obj$yhat[-(1:length(train))], test),
#     mape = MAPE(obj$yhat[-(1:length(train))], test),
#     Conf.Score = confScore(upper = obj$yhat_upper, lower = obj$yhat_lower, test)
#   )
# }
str(pfore)
```

These are not brilliant, only ever so slightly better than our baseline ARMA/mean score. This says a lot abouthow hard to beat common sense is.

## Saving the model

```{r}
str(pfore)
makeModel.proph <- function(obj){
  score = scores(obj)
  return(list(
              preds = obj$yhat,
              ul = obj$yhat_upper,
              ll = obj$yhat_lower,
              ase = score[1],
              mape = score[2],
              Conf.Score = score[3]
              ))
}
models$prophet <- makeModel(pfore)
```

# Discussion of Multiseasonal models as a whole

Considering the nature of our data, and our background knowledge of central heating in china, both of these multiseasonal models seem to be more or less appropriate. They both handle this seasonality in a similar manner, and both proved a very reasonable forecast.

# Multivariate Setup

Now it is time for us to dive in to the hopefully even more powerful multivariate models. Before we do so, we are going to need to redefine our train and test sets, which means we are going to have to write some more functions. Instead of confusing ourselves, lets go ahead and clean away our train and test sets, to avoid confusion:

```{r}
rm(train, test)
```


## Functions

First, we are now going to have to convert our non time series/numerical data to daily, which means we will have to find a new method of resampling them. 
For that, we are going to find the most common categorical value in each day, and then take that. We will use the `.mode` function, defined below:

```{r}
.mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}

.mode(c(rep("cat",4), rep("dog",8), rep("moose",1)))
```

Next we are going to write a function which lumps the categorical variables into groups of each day, takes the `.mode` of each group, and returns a new vector in that format.

For the lumping, we will use the `%/%` operator, which returns an integer specifying how many times a number divides another number. For instance:

```{r}
((1:50) - 1) %/% 5
```

Then, we will use the glorious `tapply` function, which does the same thing as the apply family, except it applies the function onto a grouped variable (table apply).

```{r}
.dailyMode <- function(xs){
  out <- tapply(xs, (seq_along(xs)-1) %/% 24, .mode)
  out <- unname(out)
  out[1:1826] # the number of days in 6 years, because we cleaned away extra days
}
```

Next we are going to write a function that applies this (and our daily time series conversion) onto a list of time series objects and categorical, and spit out a data frame:

```{r}
dlist <- function(xs){
  xs %>>%purrr::keep(is.ts) -> tsbj
  xs %>>% purrr::discard(is.ts) -> tsnobj
  out <- append(lapply(tsbj, function(x) window(cleandays(x), end = 6)), 
                lapply(tsnobj, .dailyMode)) 
  as.data.frame(out)
}
```

Finally, we are going to write a function that creates a train set and a test set of data frames of time series and categorical variables. Note the use of the `<<-` superassignment operator. This causes the value to be stored one level out, so if it is a single level function, this will cause the value to be stored in the global environment:

```{r}
dfsplit <- function(df,n = 5){
  index <- length(window(df[[1]], end = n))
  tsdf <- df %>% purrr::keep(is.ts)
  nodf <- df %>% purrr::discard(is.ts)
  outs <- lapply(tsdf, function(x) window(x,end = n))
  outn <- nodf[1:index,]
  train<<-as.data.frame(append(outs,outn))
  tests <- lapply(tsdf, function(x) window(x, start = n))
  test <<- as.data.frame(append(tests, nodf[(index):nrow(nodf),]))
}
```

Note we have to do things in this weird way, because otherwise it is extremely difficult to preserve the class `ts`, which we would like for time series data to be in. Just cbinding or anything like that breaks it immediately.

# Multivariate EDA

First, lets load up our data:

```{r}
bj <- (china$BeijingPM_ )
bj2 <- dlist(bj)
dfsplit(bj2)
```

Before we begin our analysis, it is definitely important to do more EDA, as we have a whole lot more information. First, lets plot all the time series in the training dataframe:

```{r, message = F, warning = F}
library(tidyverse)

plotAllTs <- function(df){
    df %>% keep(is.ts) %>% 
    gather() %>% 
    mutate(.data = ., t = rep( 1:nrow(df), (nrow(.)/nrow(df)))) %>%
    ggplot(aes(x = t, y = value)) +
    facet_wrap(~ key, scales = "free") +
    geom_line() + 
    theme_hc() + theme(axis.text.y = element_blank())
}
plotAllTs(train)
```

Already, we see some interesting stuff. First of all we probably want to get rid of the other stations observing particulates in our analysis, as this isnt very interesting. Of all the time seres here, humidity appears to have a similar trend pattern to our target. Pressure looks similar but a bit off, so at a high lag maybe it is interesting. Dewpoint has the wrong period, and lprec and precipitation are the same but hard to glean anything out of.

## Analysis of wind direction:

Lets see if the wind direction is of any use to us:

```{r}

ggplot(train ) + 
  geom_point(aes(x = No, y = PM_US.Post, color = cbwd)) + 
  geom_line(aes( x = No, y = PM_US.Post, alpha = 0.1 )) + 
  geom_smooth(aes(x = No, y = PM_US.Post), method = "auto") +
  theme_hc() + 
  scale_color_hc()
```

Just upon a glimpse this is not that useful. Lets look at a table:

```{r}
train %>% arrange(cbwd) %>% 
  group_by(cbwd)%>%
  summarise(mean = mean(PM_US.Post))
```

It looks like it is an interesting factor to some degree, but at this scale with our mode technique, it is probably strongly distorted. Still it is worth a look. We will note this and examine it when we are performing our models.

## Analysis of all numeric variables

```{r, results = "hide"}
plot_vs_response <- function(x){
  plot(train$PM_US.Post ~ train[[x]], xlab = x)
  lw1 <- loess(train$PM_US.Post ~ train[[x]])
  j <- order(train[[x]])
  lines(train[[x]][j],lw1$fitted[j],col="red",lwd=3)
}


train %>% purrr::keep(is.numeric) %>% names -> numNames
numNames <- numNames[-c(1:4, 11:17)] 

par(mfrow = c(2,3))
lapply(numNames, plot_vs_response)
```

So all of these appear to be fairly interesting i think. However, the summing technique we performed does not make these plots easily interpretable. Mathematically however, they likely still work out, as it is the relationship that matters. When we do linear regression, whether or not we have the total temperature sum or the mean (sum/n) should not make a difference in the significance of the relationships, only the slopes. This means we should be ok. Precipitation does not look useful. 

## CCF analysis

Lets look at the cross correlation between all the variables next:

```{r}
ppm <- train$PM_US.Post
ccfplot <- function(x){
  ccf(ppm,train[[x]],main = x)
}
ccfs <- lapply(numNames,ccfplot)
```

It appears that low lags are where these variables have the most cross correlation, other than temperature, which seems to have a bigger CCF at a pretty far from zero lag.

Next, to further our EDA, lets perform a cluster analysis of the time series data

# Time Series Clustering

There are two types of clustering techniques: Subsequence clustering, and multivariate clustering. Thankfully for us, it has been [definitively shown](https://www.cs.ucr.edu/~eamonn/meaningless.pdf) that subsequence clustering (mostly used for anomaly detection, but also for determining if we should break a time series into multiple time series for forecasting) is completely useless. However, multivariate time series clustering is still a valid and exciting technique.

Multivariate time series clustering is commonly used in the world of quantitative finance, to determine which stocks react to other which other stocks. By finding out if X stock is associated with Y stock, they can do things to make money. With further research, they can determine the nature of the relationship between X stock and Y stock. 
If X stock grows with Y stock, they can time their investments so that when they see Y stock grow, they do something that makes them money with X stock. If the relationship is negative, they can bet on Y stock (which is growing) and short X stock.

We can attempt to use this clustering to get a better idea of how our time series are associated with each other.

## More info on clustering time series

[source](https://rdrr.io/cran/dtwclust/f/inst/doc/dtwclust.pdf)

### Types of time series clustering

#### Hierarchical Clustering

Hierarchical clustering works in one of 2 ways:

1. Each item is given its own group. Groups are compared by a similarity metric. Good groups are merged. This is tried over and over until the clusters do not get better.

2. All the items are put in a group. The group is divided until it cannot be divided more without messing up the clusters

#### Partitional Clustering

Partitional Clustering kind of works the opposite of hierarchical clustering. With partitional clustering, you specify the number of groups, and it then solves the problem of maximizing inter cluster distance while minimizing intra cluster distrance. An example of this is kmeans.

#### Fuzzy Clustering

Fuzzy clustering is similar to partitional clustering, except instead of *hard* clusters, they are *soft* or fuzzy. This means that items can be in multiple clusters, but more in one cluster than another. An example of this is c-means.

### Time series distance measuers

#### Dynamic Time Warping

```{r, echo = F, fig.width = 4}
knitr::include_graphics("dtw.png")
```

Dynamic time warping, or DTW, pictured above, finds the best way to warp two time series to line up with each other over time. It creates a matrix which has dimensions m*n, where m and n are the size of two different time series. It does this for every possible pair. Then, for each matrix, it finds a path of indices which has the minimum number of steps and costs the least to line the two time series up. The distance measure is then computed based on the the number of steps it took to line the two up, weighted by cost. For more info please see the aforementioned paper

#### Shape-Based Distance (SBD)

Shape based distance is more or less a cheaper version of DTW. What it does is it takes the cross correlation between the two time series (with normalized coefficients), makes a series out of that, fourier transforms that, and calculates the distance based off of that fourier series. 


#### Global Alignment Kernel (GAK)

A global alignment kernel is another powerful distance measure (albeit really computational expensive). Instead of calculating a matrix of costs, and then running through that (relatively cheap), it calculates a set of all possible alignments and minimizes that, and then returns a similarity measure based on that alignment. Its a bit complex but the aforementioned paper explains it nicely.

### Centroids

#### Shape extraction

Shape extraction works very similarly to SBD. It picks a random time series as a centroid, and uses SBD to match it to the other time series optimally. As mentioned in the paper, it uses an eigenvector of a matrix made wit SBD aligned series

#### DTW barycenter averaging (DBA)

This centroid is represented by the average of a bunch of points grouped by their DTW alignments. It select a random centroid series, and with each iteration, it calculates the DTW alignment with each series in the cluster and the centroid. This is repeated until convergence is met.

#### Partition around mediods (PAM)

A mediod is just a time series whose average distance to all the other objects in the cluster is at the smallest (and an element of the original data). Then, that cluster is partitioned around all the mediods iteratively. IN our case, a given number of series are chosen as initial centroids. Then all the distances are calculated, and then clusters are created based off of those distances. Then, it picks the one that is closest to all the other time series in the new clusters as a mediod, and does it again, until convergence

## Moving Forward

Now that we have a basic understanding of how time series clustering works, we can try it out.

## Clustering setup

```{r, echo = -2}
library(dtwclust)
load("analysis/daily/grids.Rda")

traints <- as.list(keep(train,is.ts))
```

Now that we have our data and libraries loaded, lets set ourselves up for parallel computation:

```{r, eval = F}
# alter for the appropriate number of cores for you
# my computer has 12, but i have a lot of tabs open
# define the cluster
workers <- makeCluster(10L)
# load the necessary libraries onto the cluster
invisible(clusterEvalQ(workers, library(dtwclust)))
# register the cluster for parallel computation
registerDoParallel(workers)
```

## Partitional Clustering

As we are new to this, lets set ourselves up for a grid search of a bunch of clusters. We will first check out a partitional cluster. For this, we want to use the `dtw` distance metric, but also check out the `DBA` centroid. We will also play with their normalization methods, and the time window they use in alignment. we will also try out 2-6 clusters

### Grid search setup

```{r, eval = F}
cfg <- compare_clusterings_configs(
  types = "partitional",
  k = 2L:6L,
  controls = list(
    partitional = partitional_control(iter.max = 40L)
  ),
  distances = pdc_configs("distance",
    partitional = list(
      dtw_basic = list(
        window.size = seq(from = 5L, to = 50L, by = 5L),
        norm = c("L1", "L2")
        )
      )
    ),
  centroids = pdc_configs("centroid",
    share.config = c("p"),
    dba = list(
      window.size = seq(from = 5L, to = 50L, by = 5L),
      norm = c("L1", "L2")
      )
    ),
  no.expand = c("window.size","norm")
)
```

For evaluation, as we do not have labeled data, we will use silhouette validation as our metric.

```{r, eval = F}
evaluators <- cvi_evaluators("Sil")
```

### Partitional Grid Search

Lets find the best cluster now:

```{r, eval = F}
comparison <- compare_clusterings(
  traints, 
  types = "partitional",
  configs = cfg, 
  seed = 666L,
  score.clus = evaluators$score,
  pick.clus = evaluators$pick
)

stopCluster(cl)
registerDoSEQ()
```

### Partitional grid search: model picking

```{r, cache = T}
comparison$results$partitional %>% arrange(desc(Sil)) %>% head
```

Well we have it. We can explore the results more ourself, the most important part is we found two groups

### Best Cluster

We can conveniently rerun the best cluster with the `repeat_clustering` function:

```{r, cache = T}
clusters <- repeat_clustering(traints, comparison, comparison$pick$config_id)
plot(clusters)
clusts <- clusters@cluster
names(clusts) <- names(traints)
data.frame((clusts))
```

So looks like air pressure is on its own. Lets see if hierarchical clustering can help us out with a better cluster

## Hierarchical clustering

Lets try out a bunch of different hierarchical clusters. We should check out different preprocessing methods, different control methods, all the possible centroids, GAK, DTW, SBD, and different window sizes for DTW.

### Grid search setup

```{r, eval = F}
cfg2 <- compare_clusterings_configs(
  types = "h",
  k = 2L:6L,
  controls = list(
    hierarchical = hierarchical_control(method = "all")
  ),
  preprocs = pdc_configs("preproc", 
                         none = list(),
                         zscore = list()),
  centroid = pdc_configs("centroid",
  shape_extraction = list(),
  default = list(),
  dba=list()
  ),
  distances = pdc_configs("distance",
    hierarchical  = list(
      dtw_basic = list(
        window.size = seq(from = 5L, to = 50L, by = 5L),
        norm = c("L1", "L2")
        ),
      gak = list(),
      sbd = list()
      )
    ),
  no.expand = c("window.size","norm")
)

evaluators <- cvi_evaluators("Sil")
```

### Hierarchical grid search

Note we will use silhouettes for consistency

```{r, eval = F}
comparison2 <- compare_clusterings(
  traints, 
  types = "hierarchical",
  configs = cfg2, 
  seed = 666L,
  score.clus = evaluators$score,
  pick.clus = evaluators$pick
)
```

Looks like GAK and DBA centroids absolutely won. Lets visualize the results

```{r}
comparison2$results$hiera %>% arrange(desc(Sil))
comparison2$results$hierarchical %>% arrange(desc(Sil),distance) %>% 
  ggplot(aes( fill = distance, y = Sil, x = distance)) + 
  geom_boxplot()+ 
  facet_wrap(centroid~. ) +
  scale_fill_hc() + theme_hc()
```

So it looks like GAK out performed DTW, which outperformed SBD. From centroid to centroid, it really didnt matter.

### Best Cluster

```{r}
cluster2 <- repeat_clustering(traints, comparison2, comparison2$pick$config_id)
plot(cluster2)
plot(cluster2, type = "sc")
```

At this point, it is obvious that our results are not going to change with more work. We have two clusters, pressure in one and everything else in the rest.

## Discussion of results

With our clustering, we discovered a few interesting things.
Firstly, it appears that by all metrics,air pressure is really different from the rest of the series. Whether that is a good thing or bad thing remains to be seen, but regardless, it is interesting. We will have to pay attention to it as a predictor.

We also need to note how strongly humidity is clustered with our PM2.5 content.This hopefully suggests it will be another strong predictor variable. However, it is important to keep in mind that all our time series are clustered pretty closely, the largest distance is only 0.015. So our clustering may not have had any important results, but at the very least we learned something.

We will now proceed with our multivariate analysis. 

# VAR

Our first multivariate technique is going to be VAR. One of the interesting properties of VAR is what it means about the data. VAR is telling us that our time series are *endogenous*, that is to say that they all affect each other. Now with our weather patterns, this makes sense. Weather is a complex, dynamic system of complex feedback cycles, so it is natural that all the weather variables are endogenous to each other. However; let us discuss how our air particulate content relates to the weather, to determine weather a VAR model is physically appropriate

## Discussion of Appropriateness

In this section we will discuss how each of our weather patterns interplay (or dont) with our air quality.

### Humidity

#### Effect of Humidity on Air Quality

Humidity has a strong effect on air quality. On a humid day, water in the air will "stick" to air particulates. This weighs them down and makes them much larger, causing them to stick around for longer. This will cause the air particulates to hang around in one place. 

It is also important to note that this has an effect on temperature. When water sticks to air pollution, energy from the sun is reflected off of the water, imparting a bit of energy to the water and spreading the sunlight some more. This adds to the haze appearance when there is a lot of pollution. Over time, due to water's high specific heat, should cause the air to heat up, as the small amount of extra energy stored in the water attached to the pollution will be transferred quickly to the air. (Note this should also have an affect on pressure slowly, because $P \propto T$).

#### Effect of Air Quality on Humidity

Surprisingly, air quality also has a positive feedback with humidity. This is due to the "sticking" effect we discussed earlier. Heavy air particulate matter with water attached causes other air particles to stick around too. You can actually prove this with an experiment.


Supplies

* An aerosol, such as hairspray

* A jar with a lid

* Ice

* Warm water

Take the warm water and put it in your jar. Put the lid on upside down with ice in it, for about 30 seconds (this causes the evaporated water in the air to condense a bit, raising the humidity). 

Now, lift the ice lid up, and spray a tiny bit of your aerosol in the jar and quickly replave the ice lid. 

Then, watch a cloud form inside the jar.

You can do this experiment with any air particulate, for example smoke from a match also works nicely.

It is safe to say that a bidirectional VAR forecast of humidity is appropriate.



### Temperature

#### How Temperature Relates to Surface Air Quality

Temperature has a clear effect on air quality. Not only does cold air cause air particulates to stick around more (slower moving), but also more condensed water particles in the air, which we already know about.

Similarly, in the winter, tropospheric temperature inversions can occur when the stratosphere (think, up high) gets heated more than the troposphere (we live here). Normally, the air naturally convects because it is hotter in the troposphere than the stratosphere, but in the winter, with our long nights, the opposite can occur. This is a temerature inversion. This causes the convection to stop, and the air to remain trapped there. This traps the pollutants in the same place, and causes them to accumilate

#### Effect of air quality on temperature

The air quality + humidity should have a slight effect on temperature, but global weather patterns are more powerful

#### Discussion

It is appropriate to say that temperature affects air quality, but only slightly appropriate to say the opposite

### Dewpoint

Dewppoint is simply a function of humidity and temperature, so this variable will not be included in our model, as it is simply overfitting.

### Wind speed

This is a complex relationship, but basically it does have some sort of complex relationship with air pollution. When it is windy, pollutants can be moved thousands of miles, while when it is still, they can accumulate. So it depends, but it does have an effect. Heavy particulte matter may also slow the wind down, but it is unclear and unproven. Overall though, it is somewhat appropriate to do this bidirectional forecast.

### Pressure

Given the complex relationship between pressure, temperature, humidity, and wind speed, which are all somewhat related to air quality in interesting ways, it is safe to include pressure in our VAR models as well

### Precipitation

No matter how much i think, I cannot say that you can predict surface air quality with precipitation or vice versa, so this is likely inappropriate


## S3 methods

We need three S3 methods here, an `as` method, an `autoplot` method, and a `scores` method:

```{r}
as.var <- function(x) structure(x, class = "var")

autoplot.var <- function(obj){
  us <- obj$fcst$PM_US.Post
  testdf <- data.frame(type = "actual", 
                       t = seq_along(test$PM_US.Post), 
                       ppm = as.numeric(test$PM_US.Post))
  preddf <- data.frame(type = "predicted", 
                       t = seq_along(test$PM_US.Post), 
                       ppm = ( obj$fcst$PM_US.Post[,1]))
  dfl <- list(testdf,preddf)
  confdf <- data.frame(t = seq_along(test$PM_US.Post), upper = us[,3], lower = us[,2])
  .testPredPlot(dfl)+ geom_line(data = confdf, aes(x = t, y = lower, alpha = 0.2), 
         linetype = 3003) + geom_line(data = confdf, aes(x = t, y = upper, alpha = 0.2), 
         linetype = 3003) + guides(alpha = FALSE)
}
scores.var <- function(obj){
  mape <- MAPE(obj$fcst$PM[,1], test$PM_US)
  ase <- ASE(obj$fcst$PM[,1], test$PM_US)
  us <- obj$fcst$PM_US.Post
  conf  <- confScore(upper = us[,3], lower = us[,2], test$PM_US)
  c("ASE" = ase,"MAPE" = mape , "Conf.Score" = conf)
}

```

## Order Selection:

First, lets select the possible orders for our VAR forecast

```{r}
train[1:3]  <- NULL
train[9:14] <- NULL
test[1:3]  <- NULL
test[9:14] <- NULL
traints <- (purrr::keep(train,is.ts))
traints <- (purrr::keep(train,is.ts))
library(vars)

# simple order
ord <- VARselect(traints, lag.max = 20, type = c("both"))

# Seaosnal order
ords <- VARselect(traints, lag.max = 20, type = "both", season = c(7,365))

# Another order
ordn <- VARselect(traints, lag.max = 20)
```

Lets check out what the orders are:

```{r}
data.frame(rbind(ord$selection, ords$selection, ordn$selection), row.names = c("ord","ords","ordn"))
```

## Model Tuning

Next, lets pick the proper parameters with a grid search. First, lets build the grid:

```{r}
types <- c("const","trend","both","none")
ics <- c("AIC", "HQ", "SC", "FPE")
ps <- c(8,3,1)
szn <- c(NULL, 365)
init <- list(ps = ps, types = types, ics = ics, szn = szn)
grid <- expand.grid(init, stringsAsFactors = FALSE)
```

Then lets set ourselves up for parallel processing:

```{r, eval = F}
workers <- makeCluster(11L)
invisible(clusterEvalQ(workers, library(vars)))
registerDoParallel(workers)
```

Finally, lets run the grid search

```{r, echo = F}
load("analysis/daily/varsear.Rda")
```

```{r, eval = F}
gridSearch <- foreach(i = 1:nrow(grid), .combine = rbind) %dopar% {
  model <- VAR(traints,p = grid[i,1], type = grid[i,2], ic = grid[i,3], season = grid[i,4])
  preds <- predict(model, n.ahead = 366)
  pred <- preds$fcst$PM_US.Post[,1]
  ASE <- mean( (test$PM_US.Post-pred)^2 )
  c(ASE = ASE, p = grid[i,1], type = grid[i,2], ic = grid[i,3], season = grid[i,4])
}
stopCluster(workers)
registerDoSEQ()
```

Lets check out the results:

```{r}
gridSearch %>% as.data.frame -> gridSearch
gridSearch$ASE <- as.numeric(gridSearch$ASE)
gridSearch[2:4] <- lapply(gridSearch[2:4], factor)
gridSearch %>% arrange(ASE)
```

Next lets visualize them

```{r}
library(latticeExtra)
gridV <-  (cloud(ASE ~ p + type, 
                 gridSearch,
                 panel.3d.cloud = panel.3dbars, 
                 scales = list(arrows = F, col = 1), 
                 xbase= 0.2, ybase = 0.2,  pretty = T,
                 col.facet = level.colors(gridSearch$ASE, 
                                          at = do.breaks(range(gridSearch$ASE), 20),
                                          col.regions = cm.colors),
                 colorkey = list(
                                 col = cm.colors, 
                                 at = do.breaks(range(gridSearch$ASE), 
                                                20)),
                 screen = list( z = 40, x = -30 )
                       )
)
gridV
```

## VAR Model: parameter selection and forecasting

We will now go through and select the proper parameters for the model, by calculating the ASE of each VAR prediction, as well as looking at the summary of a linear model (to see significant regressors and help us pick), as well as thinking about our appropriateness assessment:

```{r}
var1 <- VAR(traints, p = 3, type = "none", ic = "AIC", season = 365)
preds <- predict(var1, n.ahead = 366)
pred <- preds$fcst$PM_US.Post[,1]
mean( (test$PM_US.Post-pred)^2 )
plot(preds)
summary(lm(data = traints, PM_US.Post ~.))

traints %>% select(PM_US.Post, HUMI, DEWP, TEMP, precipitation, PRES, Iws) -> trainsimp
var2 <- VAR(trainsimp, p = 3, type = "none", ic = "AIC", season = 365)
preds <- predict(var2, n.ahead = 366)
pred <- preds$fcst$PM_US.Post[,1]
mean( (test$PM_US.Post-pred)^2 )

traints %>% select(PM_US.Post, HUMI,  TEMP, precipitation,  PRES, Iws) -> trainsimp2

var3 <- VAR(trainsimp2, p = 3, type = "none", ic = "AIC", season = 365)
preds <- predict(var3, n.ahead = 366)
pred <- preds$fcst$PM_US.Post[,1]
mean( (test$PM_US.Post-pred)^2 )

traints %>% select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> trainsimp3

var4 <- VAR(trainsimp3, p = 3, type = "none", ic = "AIC", season = 365)
preds <- predict(var4, n.ahead = 366)
pred <- preds$fcst$PM_US.Post[,1]
mean( (test$PM_US.Post-pred)^2 )
plot(preds)
```

We have now reached a point where we cannot improve our model any further (trust me i tried). Lets remember these variables for the rest of our multivariate analysis. 

## Model assessment

```{r}
vpred <- as.var(preds)
autoplot(vpred)
data.frame(t(scores(vpred)))
```

Objectively, this model performed worse than the ARMA/mean baseline model. However, visually, it was only off by a bit. It seems to have gotten the period a bit wrong, while it is accurate in scale. It also has a few issues with the shape, but the forecast does not look unreasonable. This may be an interesting model in our bagged results.

## Saving the Model

```{r}
makeModel.var <- function(obj){
  score = scores(obj)
  us <- obj$fcst$PM_US.Post
  return(list(
              preds = us[,1],
              ul = us[,3],
              ll = us[,2],
              ase = score[1],
              mape = score[2],
              Conf.Score = score[3]
              ))
}

models$var <- makeModel(vpred)
```

# Neural Network models

We will try two different neural networks, a neural net ar (nnetar) model and a Long short-term memory (LSTM) model. 

The nnetar model is a simple artificial neural network (ANN). For this we will use the `nnetar` function from the `forecast` library. It is still an ANN, just as the ones from nnfor, and it is significantly less customizable than the models from `nnfor`. However, with this much data, and the slowness of `nnfor`, it actually takes over 7 hours to produce a model with our data using `nnfor`. As I plan on eventually deploying this model, and am not going to be spending months tuning a slow `nnfor` model to find the right hyperparameters (with that slowness, I will simply be using near defaults), there is no benefit from using the slower, more customizable model.

A LSTM model is much more complex than the nnetar model, but will hopefully provide us with a better view of the data. We will explor both in depth in the following sections

# ANN Model: nnetar

For the first attempt at neural network forecasting, we will be using nnetar. Nnetaris basically a less flexible implementation of the functions in the `nnfor`. What it lacks in flexibiility it makes up for with extreme speed. We will have our slow model in the next section. For discussion on the comparison of `nnfor` and `nnetar`, please see [this link](https://kourentzes.com/forecasting/2017/02/10/forecasting-time-series-with-neural-networks-in-r/). 

## What does the nnetar function do?

Basically, the `nnetar` function takes in your data, properly scales it for the neural network. We will discuss scaling extensively in the next section, do not worry. After scaling, if needed, it calculates the AR order of the time series, as well as the lags. Then, it takes your scaled time series, the things it calculated and adjustements it made, into the `nnet` function from the nnet library. Then, you have a model and you make predictions. It is very user friendly, and very very fast.

## S3 Methods

We will use standard, unremarkable coercion, autoplotting, and scoring methods for the neural net forecasts.
```{r}
as.nfor <- function(x) structure(x, class = "nfor")

scores.nfor <- function(obj){
  mape <- MAPE(obj$mean, test[[1]])
  ase <- ASE(obj$mean, test[[1]])
  confs <- confScore(upper = obj$upper[,2], lower = obj$lower[,2], test[[1]])
  c("ASE" = ase,"MAPE" = mape,  "Conf.Score" = confs)
}
autoplot.nfor <- function(obj){
  testdf <- data.frame(type = "actual", 
                       t = seq_along(test[[1]]), 
                       ppm = as.numeric(test[[1]]))
  preddf <- data.frame(type = "predicted", 
                       t = seq_along(test[[1]]), 
                       ppm = as.numeric( obj$mean ))
  confdf <- data.frame(t = seq_along(test[[1]]), upper = obj$upper[,2], lower = obj$lower[,2])
  dfl <- list(testdf,preddf)
  .testPredPlot(dfl)+ geom_line(data = confdf, aes(x = t, y = lower, alpha = 0.2), 
         linetype = 3003) + geom_line(data = confdf, aes(x = t, y = upper, alpha = 0.2), 
         linetype = 3003) + guides(alpha = FALSE)
}

```

## Fitting the model

This is much simpler and faster than the nnfor `mlp` and `elm` function, and without significant tinkering or expertise. Lets check it out.

```{r, eval = F}
traints %>% dplyr::select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> train
nnet <- nnetar(train[[1]], xreg = train[-1],repeats = 1000)
```

```{r, echo = F}
load("analysis/daily/nnetar.Rda")
```

Lets check out what the model did

```{r}
data.frame(summary(nnet))
```

Interesting. We see that it chose an AR(1) our data. Lets look more closely at the model and the method to see what else is going on.

```{r}
nnet$model
nnet$method
```

Pretty simple. 32 input nodes, 16 nodes in the hidden layer, and then collapsed into the output node. We can also see the details of our model. It is a `NNAR(27,1,16)[365]`. That is read as a neural network model, with the data lagged 27 times, AR(1), and 16 nodes in the hidden layer.

## Forecasting

The nnet models do not have the same issue as the other forecast models, so we can forecast without an s3 method

```{r}
test %>% dplyr::select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> test
```

```{r, eval = F}
nnetfor <- forecast(nnet, h = 366, xreg = test[-1], PI = TRUE)
```

```{r, echo = -1}
load("pres/nnetfor.Rda")
autoplot(nnetfor) + theme_hc()
```

## Model evaluation

Lets take a closer look at the forecast:

```{r}
nfor <- as.nfor(nnetfor)
autoplot(nfor)
```

Visually, this is not a bad forecast. We managed to capture a lot of the behavior of the time series, and even neared it on a few of the peaks. Lets check out the scores to see. I imagine the MAPE will be very good in this case.

```{r}
data.frame(t(scores(nfor)))
```

Looks like this one did about the same, maybe slightly better, than the common sense ARMA benchmark, in terms of ASE. However in terms of MAPE, this is far and away our best model. This means that in general, it was pretty close to the truth. However, when it missed, it missed by a lot. This is evidenced in our plot as well. We will now move to the more complex, and interesting, LSTM model

## Saving the model

### How is Prediction Interval Calculated for an ANN/MLP

Please refer to this chapter of [Forecasting: Principles and Practice](https://otexts.com/fpp2/nnetar.html).

```{r}
makeModel.nfor <- function(obj){
  score <- scores(obj)
  return(list(
          preds = obj$mean,
          ul = obj$upper[,2],
          ll = obj$upper[,2],
          ase = score[1],
          mape = score[2],
          Conf.Score = score[3]
          ))
}
models$nnetar = makeModel(nfor)
```

# Long Short-Term Memory (LSTM)

LSTM models are a type of recurrent neural network. Before we get into the actual modeling, we need to stop and think about what this means.

The model in this section is the result of hours and hours of hard work (over 700 models were trained by hand), and can be used, in its trained form, in `analysis/daily/winner.h5`.

To load and test on your own dataset, you simply need to get the data in the right format (I will work on automating this later, for a fully reproducible, deployable model), and run `lstm_model <- load_model_hdf5("/path/to/winner.h5")`, and the fully trained model will pop up in your global environment.will pop up in your global environment.


## What is a recurrent neural network(RNN)?

To explain this, we need to discuss what a normal neural network, a feedforward neural network is. In a feedforward neural network, inputs go into the input layer, is mapped through an activation function, goes to some number of hidden layers with their own activation functions. The hidden layer finds a numerical way to represent the data that was fed into it, and then this representation is passed into the output layer. Information is fed straight in, and straight out. This means that a simple NN has no way of discerning anything about the order of things, or time. They know nothing about previous iterations. 

In contrast, in an RNN, the inputs are not just the data, but the previous outputs of the RNN. This means it has a sort of memory about what it did in the past, and can learn from it. This also means they have a notion of order and time, and will do well for dealing with time series. The information of previous iterations is stored in the RNNs hidden layers, so with each iteration they are aware of the past. They are especially good at finding correlations between events which occured a long way apart. 

> An RNN is a neural network that shares weights it has created over time

## What is an LSTM?

An LSTM is a special class of RNN, where extra (outside of the normal information flow of a RNN) information is stored in a gated cell. The cell is updated and changed by the RNN. What happens is the signal from inside the RNN is passed through a sigmoid funciton, and then based on the strength and weight of that signal, either allow it to be stored in the long term memory cell. This functional method allows for backpropogation of errors (it can remember the error of previous steps, and allow that to help the gradient boosted optimization of the network (we will discuss this)).

There are three gates in the LSTM: 

1. The input gate
   * Information from the inputs is stored here

2. The forget gate
   * Information from the hidden layer is used to update this gate. It is allowed to both remember and forget information. This is what separates the LSTM from a GRU

3. The ouptut gate
   * Information at the output layer is stored here

Thats the gist of it. We will get more into it as we build our model.


For an ***extremely*** helpful discussion of neural networks and especially RNNs, please refer to [this link](https://skymind.com/wiki/lstm#feedforward)

Please also note that the code in this section is based off of the code found in the excellent book `Deep Learning with R`, an excellent investment.


## Data preparation

As LSTMs rely on the sigmoid function, which is between zero and one, we must prepare our data accordingly (aka center and scale it). It also has to be in matrix form (for now). We also want the LSTM to see the lagged version of our variables. We are going to lag not only the xreg variables, but also our target data itself. This allows the neural network to do a sort of autocorrelation analysis of the data, as well as analysis of the cross corelation at many lags. For this, we will use the `mlag_dfr` function from my `tswgewrapped` package. This takes as an input column names, a vector of lags, and then adds those columns to the data frame. As we have tons of data, and are probably only going to go to about lag 50, we will deal with the NAs created in our typical manner: spline interpolation. Lets check out the `mlag_dfr` and `lag_dfr` functions:

```{r}
tswgewrapped::lag_dfr
tswgewrapped::mlag_dfr
```

Now lets do the initial preprocessing of our data

```{r, results = "hide"}
library(keras)
# reset the data state of this report
rm(train, test, traints)
dfsplit(bj2)

# unneccessarily remove columns we arent using
train[1:3]  <- NULL
train[9:14] <- NULL
test[1:3]  <- NULL
test[9:14] <- NULL
traints <- (purrr::keep(train,is.ts))
test %>% dplyr::select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> test
traints %>% dplyr::select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> train
test <- mlag_dfr(test, names(test), 1:50)
train <- mlag_dfr(train, names(train), 1:50)
test <- impute(test) %>% as.data.frame
train <- impute(train) %>% as.data.frame
train <- data.matrix(train)
test <- data.matrix(test)

# make an overall data matrix
dat <- rbind(train,test)

# We have one too many columns because of the nature of the ts objects (with their windows, 
# this is fine for forecasting but not for an LSTM)
dat <- dat[-1,]

# do not worry about this, we are doing this so later on when we do 
# our bagging, we have a validation technique
# it will all make sense i promise

maxt <- floor( nrow(dat)*4/5 )
maxv <- floor(nrow(dat))
val <- dat[(maxv-365*2):( maxv - 365),]
val <- scale(val, center = apply(val,2,mean), scale = apply(val,2,sd))
valScale <- attr(val, 'scaled:scale')[1]
valCent <- attr(val,'scaled:center')[1]
val <- array(val, c(nrow(val),2,255))

# normalize the data for the lstm
mn <- apply(dat,2,mean)
std <- apply(dat,2,sd)
dat <- scale(dat, center = mn, scale = std)

# lets also make an array for our test dataset
# in the form of our input to the lstm, which we will discuss next
# Note we are doing this in a convoluded manner because we want to be able
# to denormalize our data. Next time i will use the recipes package
# because this is a massive headache

test2 <- scale(test, center = apply(test,2,mean), scale = apply(test,2,sd))
test3  <- array(test2, c(nrow(test2),10,255))
```

That was a lot of code, but we did nothing fancy. Next, we are going to write a `generator function`. 
This keeps us from having to load our data into the GPU's memory (if you are using a GPU). 

## Generator function

As mentioned above, if you are running GPU based calculations, you often dont want to load an entire dataset in your GPU's memory. So, it is good practice to write a generator function. What this will do is create another function. It will generate samples, moving forward through time, on the fly. As we want to move through the time series slowly, with overlap, there would be a massive amount of data to store. We need to define three values for this:

***Lookback*** how far back we will go to grab our observations
***steps*** How far apart our samples will be
***delay*** How far ahead our targets will be from the observations
***batch_size*** How many times we do this per batch

```{r}
lookback  <- 60 # observations will go back 60 days
steps <- 6 # We will sample every 6 days
delay  <- 1 # we will set our prediction targets to be 1 day ahead
batch_size <- 150 #the number of times we will do this per batch
```


Next, we write a series of functions to generate these samples for our model

```{r}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }

    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                      
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]]-1,
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }           
    list(samples, targets)
  }
}


train_gen <- generator(
  dat,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = maxt,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  dat,
  lookback = lookback,
  delay = delay,
  min_index = maxt+1,
  max_index = maxv,
  step = step,
  batch_size = batch_size
)
```

```{r, echo = F}
model_lstm <- load_model_hdf5("analysis/daily/winner.h5")
```

## Model Construction 

We will discuss now the construction of our model. Our final model consists of 6 layers in this case, 5 LSTM layers and one standard densely connected neural network. We will discuss each unique layer in some detail, as extreme measures were necessary to avoid overfitting with such a small amount of data

### How Does Training an LSTM Work?

Training an LSTM works as follows:

1. You define the number of batches per epoch. An ***epoch*** is a single training run for a neural network. So you define the number of times your generator function samples the data per epoch

2. In each epoch, the neural net first reads in your training data, and trains on that, using whatever masking/dropout you have (we will discuss this). Then, when it is done with that, it runs the validation (test set in this case) data through the neural net, and compares results. On the validation set, it removes any predefined masking or dropout (which is why the loss is generally lower on the validation set). Things that are confusing now will be discussed later, as it makes more sense to define with an example. Basically, each epoch it runs through the training data N steps, with some tricks to prevent overfitting the training data. Then it runs through once with you validation/test set. With each step of this, weights for each parameter are stored in the nodes themselves, and other useful information such as error is stored in the gates. The weights and the info in the gates gets added to (and removed from in the case of the forget gate). This is done N steps, over M epochs, repeatedly updating the LSTM's weights and other information until we say so. 

### Simple LSTM Layer

The first layer consists of an LSTM with 10 hidden nodes. A small amount of nodes at the beginning was necessary to avoid overfitting (even at 50 nodes we rapidly overfit). The next tool we used to combat overfitting was dropout. There are two types of dropout. First we will discuss normal dropout:

#### Normal Dropout

Normal Dropout is used in the input and output layers of a neural network. For a given dropout rate N, it uses matrix multiplication to randomly remove N percentage of the data going in to the node. This is NOT used on the validation set. By showing the neural network less points of the significantly larger training set, it prevents overfitting on the training set

#### Recurrent Dropout

Recurrent Dropout is the exact same thing as normal dropout, except it affects the hidden layer instead of the input and output layers

Our simple LSTM layer just has these parameters

### Bidirectional LSTM Layers

Next we pop the output of our first LSTM layer into two BiLSTM layers. A normal LSTM passes the data in from start to end. A ***bidirectional*** LSTM actually represents two LSTMs in parallel, one that reads the data from start to end, and one that reads the data from end to start. This helps it find even more interesting patterns and nuances in our data, increasing its representational power. Essentially, we are going to allow the LSTM to track long term patterns with equal power as short term powers with this. 


### The Dense Layer

A dense layer is a standard neural network. Adding this to our model further increases the representational power.

## The Model

We will now define the model:

```{r, eval = F}
model_lstm <- keras_model_sequential() %>%
  layer_lstm(units = 10, dropout = 0.4, recurrent_dropout = 0.1, 
             activation = "sigmoid",
             input_shape = list(NULL, dim(dat)[[-1]]),
             return_sequences = TRUE) %>%
bidirectional(
              layer_lstm(units = 40,  dropout = 0.4, recurrent_dropout = 0.4,
                         activation = "sigmoid", return_sequences = TRUE)
              ) %>%
bidirectional(
              layer_lstm(units = 80,  dropout = 0.4, recurrent_dropout = 0.4,
                         activation = "sigmoid")
              ) %>%
  layer_dense(units = 1)
```

Lets see what the model looks like now

```{r}
model_lstm
```

## Compiling the model

Next, we need to compile the model, or define how it trains. We will want to train it to optimize ASE, as we have been using that metric on the rest of the models. For our optimizing method, we will use `rmsprop`. To learn how RMSprop works, please see[this link](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/). It is typically a good optimizer for RNNs. 

The parameter to really tune here is learning rate. We will see in the next section how to tune it. By default, the learning rate is `0.001`.

```{r, eval = F}
model_lstm %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mse"
)
```

## Training the model

Finally, we get to train our model. We will save the history of our training, so we can diagnose the model

```{r, echo = F}
load("analysis/daily/LSTM.Rda")
load("analysis/daily/lstmHist.Rda")
```

```{r, eval = F}
lstmHist <- model_lstm %>% fit_generator(
  train_gen,
  steps_per_epoch = 40,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

## Model Diagnostics

Lets look at the loss curve of our training to see how we did:

```{r}
plot(lstmHist)
```

There are two things to know about the loss curve:

1. If the validation loss is greater than training loss, you are severly overfitting.

2. If the loss is too steep, learning rate is too high. If it is too flat, the learning rate is too low

For an amazing guide about tuning this, please refer to [this link](http://cs231n.github.io/neural-networks-3/#baby)

Knowing this, we can now assess our model. First of all, thanks to our dropout, we are absolutely not overfitting. Looking at the shape, the learning rate is likely ***slightly*** too high, but this is not worth tuning. (This model performs excellently, and after several hundred tries I know it is not worth it to fiddle further).

## Forecasting

### Calculating Prediction Interval with LSTMs

As with the ANN, there is no set way of calculting the prediction interval of an LSTM, as it is not a stochastic, parametric tool, it is simply a composition of lots of functions.
However, we can use our dropout to approximate a prediction interval. For an in depth explanation, please refer to [this paper](https://arxiv.org/pdf/1506.02142.pdf). Basically, dropout is a random process. So each time we run our model on the training set, with our dropout, and our random generator function, we will get slightly different results (whereas when we make a prediction, we will not have the dropout, so it will be the same result every time). So, what we can do, is we can evaluate how well our model represents the training set, and use that to calculte a prediction interval. First, we will write a function that gets the mean error of our model over the training set:

```{r}
getMeanError <- function(mod){
  # 28 is simply the integer number of times
  # we need to sample from our training set to represent the whole thing.
  # In this case it will run through the entire set three times
  sqerror <- evaluate(mod, train_gen, 28)
  return(sqrt(sqerror))
}
```

Next, we will write a function which does this `n` times, so we have a sample of errors. First, a brief note about writing for loops in R

#### How to write a good for loop in R

First, we must allocate the space in our computer's RAM for the object we are constructing. If we do not do this, with each iteration of the for loop, R will actually destroy the object,  and then create a new one. This creates horrificly slow for loops. For more info on this, please see the amazing book [The R Inferno](https://www.r-bloggers.com/the-r-inferno-revised/). To allocate this space, we do as follows:


```r
out <- double(n) 
```

Note that we allocated it as a double. If you want to make the for loops even faster, you must help out your R interpreter. If you do not define the type of the output of your loop, with each iteration the R interpreter will have to guess the type, which prevents it from optimizing. Double is the equivalent of numeric, but makes more sense to the reader. 

Next, we must define the for loop. We will us seq_along instead of length, because if somebody defines the object to be something with length zero, seq_along will behave properly, while length will break everything

```r
for (i in seq_along(out)){
  do things here
}
```

Now, lets write our for loop function, to get the errors `n` timese:

```{r}
GetErrors <- function(n){
  errors <- double(n)
  for (i in seq_along(errors)){
    cat("evaluation number", i, "\n")
    errors[i] <- getMeanError(model_lstm)
  }
  return(errors)
}
```

Next, lets get our errors, for 1000 runs of the LSTM

```r
trainErr <- GetErrors(1000)
```

```{r, echo = F}
load("analysis/daily/trainErr.Rda")
```

Next, lets look at the errors, and get their mean and std deviation:

```{r}
hist(trainErr)
errorMean <- mean(trainErr)
errorStd <- sd(trainErr)
```

Finally, lets write a function that takes our predictions, and gives them an interval for uncertainty:

```{r}
makeInterval <- function(prediction){
  pm <- errorMean+errorStd
  upper <- prediction + pm
  lower  <- prediction - pm
  return(data.frame(fitted = prediction, upper = upper, lower = lower))
}
```

### S3 methods

We define three S3 methods for predictions made by Keras:

```{r}
as.keras <- function(x) structure(x, class = "keras")
autoplot.keras <- function(obj){
 testdf <- data.frame(type = "actual", 
                      t = seq_along(test[,1]), 
                      ppm = as.numeric(test[,1]))
 preddf <- data.frame(type = "predicted", 
                      t = seq_along(test[,1]), 
                      ppm = as.numeric(obj$fitted))
 confdf <- data.frame(t = seq_along(test[,1]), upper = obj$upper, lower = obj$lower)
 dfl <- list(testdf,preddf)
 .testPredPlot(dfl) + geom_line(data = confdf, aes(x = t, y = lower, alpha = 0.2), 
         linetype = 3003) + geom_line(data = confdf, aes(x = t, y = upper, alpha = 0.2), 
         linetype = 3003) + guides(alpha = FALSE)
}
scores.keras <- function(obj){
  c(ase = ASE(obj$fitted, test[,1]), 
    mape = MAPE(obj$fitted, test[,1]),
    Conf.Score = confScore( upper = obj$upper, lower = obj$lower, test[,1] ) 
  )
}
```

Now that we are set up, lets predict off of the test set to see how we did

```{r}
lstmTest <- predict(model_lstm, test3, n.ahead = nrow(test2))
lstmTest <- makeInterval(lstmTest)
descaleTest <- function(x){
  x*attr(test2, 'scaled:scale')[1] + attr(test2, 'scaled:center')[1]
}
lstmTest <- lapply(lstmTest, descaleTest)
lstmTest <- as.keras(lstmTest)
```

That was pretty easy. Now we see why we made these test2 and test3 objects, with the proper dimensions for our neural network. Without this it would be very difficult. The dimensions of the input array for our model are as follows:

```r
array(nrow(input), lookback/step, ncol(input))
```

## Forecast Evaluation
```{r}
autoplot(lstmTest)
```



Interesting. It matched the shape of the data excellently, or at least the overal pattern, but the peaks and troughs seem to be flattened. This is due to our slightly too high learning rate.To put it in human terms, the LSTM is learning too fast, and "skipping over" some details. 

However, again trying to guess these is a bit overkill at this length forecast, and being conservative and "pretty good" is completely appropriate. Lets check out the scores:

```{r}
data.frame(t(scores(lstmTest)))
```

Wow. This gave us the very best forecast so far. Let us try now to build an ensemble forecast. To do this, and this will be explained later, lets fit the LSTM to the second to last year of our dataset:

```{r}
lstmVal <- predict(model_lstm, val, n.ahead = nrow(test2))
lstmVal <- makeInterval(lstmVal)
descaleVal <- function(x){
  x*valScale + valCent 
}
lstmVal <- lapply(lstmVal, descaleVal)
lstmVal <- as.keras(lstmVal)
```

### Saving the model

```{r}
makeModel.keras <- function(obj) {
     score <- scores(obj)
     return(list(preds = obj$fitted, ul = obj$upper, ll = obj$lower, ase = score[1], mape = score[2], 
         Conf.Score = score[3]))
 }

models$keras <- makeModel(lstmTest)
```

## Further study

With much much more data, it would be really interesting to try and build models with:

* Convolutional Neural Networks (these are used a lot in practice)

* Hyrid of CNN and LSTM

* CNN with wavenet architecture [link](https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Conv_Intro.ipynb) (requires a lot more data)

* GluonTS: State of The Art Probabalistic Time Series Forecasting with mxnet, currently working on (and trying to write an interface with R). [link](https://paperswithcode.com/paper/gluonts-probabilistic-time-series-models-in)

* Time series classification

* More clustering


# Ensemble Methods

In the previous sections, we created several time series models of the PM 2.5 content in Beijing's air. When faced with the task of choosing just one, we find ourselves asking "do we have to choose?". Each model produced tells us a different story, and are in general somewhat useful.

> All models are bad. Some are useful - George Box

An overview of the models:

```{r}
str(models) 
```

Why would we want to pick just one, and lose the utility of the rest?
Instead, we can try combining them into an even better model

> But how?

There are many ways to combine models in data science. The simplest of which would be to just average all of our forecasts together and call it a day. However, after all this work, that would certainly be uneventful. We would like to be able to have all the strengths of our models, and less of the weaknesses. How can we do that?

## Stacked Ensembles and Super Learning

[reference](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html)

[paper](https://www.degruyter.com/view/j/sagmb.2007.6.issue-1/sagmb.2007.6.1.1309/sagmb.2007.6.1.1309.xml)

The most State of the Art (SOTA) technique for combining models in such a way that each of their benefits is reaped is known as ***Ensemble Stacking*** or ***Super Learning***. To do super learning, we need to follow these steps:

* Train a bunch of models (done)

* Make predictions on a smaller subset of the data (partially done), using the base learners

* Train a meta learning model (metalearner) on these predictions

* Put new data into the base learners, have it make predictions

* Feed those predictions into the metalearner, and have it make predictions.

By using a machine learning model to combine our models, it can pick and choose the best parts of our model, in order to create a model with even more general utility.

## Metalearners

There are three main metalearning models for stacked ensembles:

* GLM (generalized linear model)

* Gradient Boosting (and XGBoost)

* Random Forests

We will not consider the GLM, as linear models in general are not appropriate for time series data. We will give a brief overview of the other two here:

### Random Forest

A random forest works as follow

1. Choose a random subset of the features in your dataset

2. Take a random sample from the selected features

3. Fit a **fully grown** decision tree to that sample

4. Repeat steps 1-3 many (think thousands and millions) times

5. Bag (bootstrap aggregate) these together

By bagging thousands of decision trees, which tend to struggle with overfitting (low bias, high variance), the random forest gets us a model which is low  bias and low variance (aka a great model). To see how this works, please refer to [this paper](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). The random forest gets near (slightly higher) the bias of a decision tree overfit to the original data, but manages to get a significantly lower variance, leaving you with a good model. 

What does this mean to us? This means it will find the optimal way to weight our predictions, and combine them, using the power of a forest of decision trees, so that we have a new, super predictive model.

### Gradient Boosting

> If linear regression was a toyata camry, then gradient boosting would be a Blackhawk helicopter [source](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)

Gradient boosting works very similarly to the random forest, except instead of many fully grown trees in parallel, it grows many short trees (some algorithms grow decision "stumps" even) iteratively, learning from each one and trying to optimize a predefined loss function (likely mse) with each step, by using the gradient. Basically, it starts with a high bias low variance decision tree (short), then makes another one and another one, each time lowering bias further and further, and then bags them together to also attain low bias and low variance.

The way our models are combined with these two methods are the same. Let us set ourselves up.

# Ensemble modeling: Setup

For the benefit of the reader, we will not go into the tedious process of choosing which models are going to go in to the super learning algorithm. If you would like to see the model choice method, please refer to: [stacking.R](https://github.com/josephsdavid/ChinesePM/blob/master/analysis/daily/stacking.R)

The first step of our setup is to make predictions for the second to last year of our data for each of our models

## Classical Models

Only the baseline ARMA model was used in this section, as forecasting the mean with a little wiggle seemed to make our model overall better when picking base models. Forecasting the second to last year of data is trivial, and can be seen in [classicalUvar.R](https://github.com/josephsdavid/ChinesePM/blob/master/analysis/daily/classicalUvar.R).

## VAR

First, a function to split our training dataset into train and validation sets was defined:
```{r}
dfsplit2 <- function(df,n = 4){
   index <- length(window(df[[1]], end = n))
   tsdf <- df %>% purrr::keep(is.ts)
   nodf <- df %>% purrr::discard(is.ts)
   outs <- lapply(tsdf, function(x) window(x,end = n))
   outn <- nodf[1:index,]
   train2<<-as.data.frame(append(outs,outn))
   tests <- lapply(tsdf, function(x) window(x, start = n))
   validation <<- as.data.frame(append(tests, nodf[(index):nrow(nodf),]))
 }
dfsplit
```

These two functions allow us to split our training and test sets into smaller training and validation sets, as seen here:

```{r}
dfsplit(bj2)
dfsplit2(train)
test %>% dplyr::select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> test
validation %>% dplyr::select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> validation
train %>% dplyr::select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> train
train2 %>% dplyr::select(PM_US.Post, HUMI,  TEMP,   PRES, Iws) -> train2
```

Then, VAR was forecast on the validation and test sets

```{r, eval = F}
# validation model
var <- VAR(train2, p = 3, type = "none", ic = "AIC", season = 365)
preds <- predict(var, n.ahead = 366)
varval <- preds$fcst$PM_US.Post[,1]

# test model
vart <- VAR(train, p = 3, type = "none", ic = "AIC", season = 365)
preds <- predict(vart, n.ahead = 366)
vartest <- preds$fcst$PM_US.Post[,1]
```


## Prophet

I do not necessarily believe in the prophet model, as a useful forecasting tool, so it is not included.

## TBATS

same procedure, luckily this model was already defined

```{r, eval = F}
# validation
batval <- newFore(obj = bjbats,  msts(validation[[1]], seasonal.periods = c(7, 365.25)), h = 366)
batval <- fitted(batval)

# test
battest <- newFore(obj = bjbats,  msts(test[[1]], seasonal.periods = c(7, 365.25)), h = 366)
battest <- fitted(battest)

```

## Multiseasonal

Here we need to fourier expand our validation set before we can predict

```{r, eval = F}
# validation
xpansion <- fourier(msts(validation[[1]], seasonal.periods = c(7,365.25)), K = c(3,100))
mseaval <- newFore(mseadyn, newdata = msts(validation[[1]], seasonal.periods = c(7,365.25)), xreg =xpansion, h = 1)
mseaval <- fitted(mseaval)

# test
xpansion <- fourier(msts(test[[1]], seasonal.periods = c(7,365.25)), K = c(3,100))
mseatest <- newFore(mseadyn, newdata = msts(test[[1]], seasonal.periods = c(7,365.25)), xreg =xpansion, h = 1)
mseatest <- fitted(mseatest)

```

## nnetar

Here we see why we split up the validation set and got rid of useless columns in advance. This will take much less than the 8 hours it took to predict previously, as there are no prediction intervals to calculate

```{r, eval = F}

nnnetfor <- forecast(nnet, h = 366, xreg = validation[-1])
netval <- nnetfor$mean

nnnetfor2 <- forecast(nnet, h = 366, xreg = test[-1])
nettest <- nnetfor2$mean

```


## LSTM

We already did the LSTM validation and test sets in advance, thankfully, so we do not have to worry about reformatting data, or doing this again

## Combining them:

```{r, eval = F}

bagval <- data.frame(
                  ppm = validation[[1]],
                  tbats = batval,
                  VAR = varval,
                  mseas = mseaval,
                  nnetar = netval,
                  arma = armaval,
                  lstm = lstmval
)

bagtest <- data.frame(
                  ppm = test[[1]],
                  tbats = battest,
                  VAR = vartest,
                  mseas = mseatest,
                  nnetar = nettest,
                  arma = armatest,
                  lstm = lstmtest
)
```

Lets check it out
```{r, echo = F}
load("analysis/daily/bags.Rda")
bagval <- bagval %>% dplyr::select(-c(arima, aruma))
bagtest <- bagtest %>% dplyr::select(-c(arima, aruma))
```

```{r}
head(bagval)
head(bagtest)
```

# Random Forest

First, we will run a random forest on the data, without any tuning, just to see what we will get, then we will perform a grid search and tune the model, followed by calculting a prediction interval:

## Dry run

```{r}
library(randomForest)
preTune <- randomForest(ppm ~ ., data = bagval,
                        ntree = 1000, nodesize = 5, importance = TRUE)
varImpPlot(preTune)
```

Next, lets write a quick scoring method, followed by viewing the prediction

### Quick S3 method

```{r}
as.ensemble <- function(x) structure(x, class = "ensemble")

scores.ensemble <- function(obj){
  ase <- ASE(obj, bagtest[[1]])
  mape <- MAPE(obj, bagtest[[1]])
  c("ASE" = ase, "MAPE" = mape)
}
```

Next, lets make the prediction and view it:

```{r}
preTunePred <- predict(preTune, bagtest) 
preTunePred <- ts(preTunePred, frequency = 365,start = 5, end = 6)

bagtest <- as.data.frame(lapply(bagtest, function(x) ts(x, frequency = 365, start = 5, end = 6)))
autoplot(bagtest$ppm, size = 2) + 
  autolayer(preTunePred, size = 2) +
  autolayer(bagtest$tbats)+
  autolayer(bagtest$VAR) + 
  autolayer(bagtest$mseas)+
  autolayer(bagtest$nnetar)+
  autolayer(bagtest$arma) +
  autolayer(bagtest$lstm)+
  theme_hc() + scale_color_hc()
```

This looks like a ridiculous forecast. Lets check it out in more detail:

```{r}
autoplot(bagtest$ppm) + 
  autolayer(preTunePred) +
  theme_hc() + scale_color_hc()
```

And the raw scores:

```{r}
data.frame(scores(as.ensemble(preTunePred)))
```

This is an absurdly good forecast. We have already reduced our lowest ASE by nearly a million. Lets now try a grid search to optimize even further

## Parallelized Grid Search

With models like Random Forests and Gradient Boosting, it is important to get the parameters just right, in order to get the best model out of it. They are not so simple as just setting p = 5. It is the same as with the deep learning models, a lot of effort is involved with getting the best prediction. With that said, we will now tune **all** the parameters of the random forest with this data, in order to get the absolute best prediction.

```{r, echo = F}
load("analysis/daily/rfS.Rda")
load("analysis/daily/rfbag.Rda")
finalForest <- final
load("analysis/daily/bagged1.Rda")
finalForestPred <- finalPred
```

### Setting up the grid

First, we initialize a grid to search through:

```{r}

x <- 1:7000
ntree <- x[x%%50 == 0]
mtry <- c(1:length(bagtest))
x <- 1:100
nodesize <- x[x%%5==0]
nPerm <- 1:3
init <- list(
             ntree=ntree,
             mtry = mtry,
             nodesize = nodesize
)
grid <- expand.grid(init, stringsAsFactors = FALSE)
dim(grid)
```

We will be running about 20,000 random forests, so it is probably best we do our grid search in parallel, in order to save time and be efficient. My computer has 12 cores, lets use all of them:

```{r, eval = F}
workers <- makeCluster(12L)
invisible(clusterEvalQ(workers, library(randomForest)))
invisible(clusterEvalQ(workers,set.seed(19)))
registerDoParallel(workers)
```

### Running the grid search

We will use the `doParallel` and `forEach` libraries to do this in parallel, and run the search:

```{r, eval = F}

gridSearch <- foreach(i = 1:nrow(grid), .combine = rbind) %dopar% {
  model <- randomForest(ppm ~ .,
                        data = bagval3,
                        ntree = grid[i,1], 
                        mtry = grid[i,2], 
                        nodesize = grid[i,3],
  )
  preds <- predict(model, bagtest3)
  ASE <- mean((bagtest3$ppm - preds)^2)
  data.frame(ASE = ASE, 
       ntree = grid[i,1], 
       mtry = grid[i,2], 
       nodesize = grid[i,3]
  )
}
```

Lets check it out:

```{r}
gridSearch %>% arrange(ASE)
```

## Using the Tuned Model

Due to the inherently random nature of the random forest, the results may vary from time to time, but in general they should be excellent. Train our final random forest model and make a prediction:

```{r, eval = F}


```





# Reproducing this analysis

All work performed in this project is intended to be completely reproducible. The following sections describe the necessary steps to reproduce this analysis. Obviously the first step is to clone the parent repository:

```{sh, eval = F}
git clone https://github.com/josephsdavid/ChinesePM.git
```

## Reproducing the working environment

### Using Nix

The author is a huge fan of the reproducible package manager [nix](https://nixos.org/nix/), which allows the user to share a reproducible file `shell.nix`. When someone else runs from the command line `nix-shell`, the contents of the file are installed in isolation from the rest of the users system, and is dropped into a shell which perfectly reproduces the working environment of this project. 
This includes automatically installing and setting up the tensorflow/keras deep learning stack, installing complex dependencies, and if you comment out the tensorflow line and uncomment the tensorflow with CUDA line, will automatically set up tensorflow to run with Nvidia CUDA.

To install remaining dependencies (namely devtools packages), you can run `Rscript libs.R` to automatically install them. This technique is highly recommended. Below are the contents of `libs.R`:

```{r, code = readLines("../libs.R"), eval = F}
```

The `shell.nix` file can be seen in the github repository.

Note that using this method will also automatically setup tensorflow, Keras, tensorboard, and any other necesary dependencies for deep learning time series models (with R and python). 

### Without Nix

To reproduce the R dependencies required for this analysis, please run `Rscript noNix.R`
This reads `shell.nix`, searches for R packages, compares what is there to what is installed on your computer, and then if they are different, it installs them

```{r, code = readLines("../noNix.R"), eval = F}
```

Note that this method will require you to manually set up a deep learning stack

## Reproducing the models

To reproduce the models from this project, you have two options: you can either load the saved `.Rda` files, or run all the models (as shown in this report). Note this will likely take several days, as some of the models take a long time to train, and multiple grid searches over 10,000+ hyperparameters were performed.
